<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.97.2"><meta name=ROBOTS content="INDEX, FOLLOW"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Understanding Q-learning: How a Reward Is All You Need | Spice.ai blog</title><meta name=description content="There are two general ways to train an AI to match a given expectation: we can either give it the expected outputs (commonly named labels) for …"><meta property="og:title" content="Understanding Q-learning: How a Reward Is All You Need"><meta property="og:description" content="There are two general ways to train an AI to match a given expectation: we can either give it the expected outputs (commonly named labels) for differents inputs; we call this supervised learning. Or we can provide a reward for each output as a score: this is reinforcement learning (RL).
Supervised learning works by tweaking all the parameters (weights in neural networks) to fit the desired outputs, expecting that given enough input/label pairs the AI will find common rules that generalize for any input."><meta property="og:type" content="article"><meta property="og:url" content="/posts/2021/12/15/understanding-q-learning-how-a-reward-is-all-you-need/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-12-15T00:00:00+00:00"><meta property="article:modified_time" content="2022-01-04T10:49:39+09:00"><meta itemprop=name content="Understanding Q-learning: How a Reward Is All You Need"><meta itemprop=description content="There are two general ways to train an AI to match a given expectation: we can either give it the expected outputs (commonly named labels) for differents inputs; we call this supervised learning. Or we can provide a reward for each output as a score: this is reinforcement learning (RL).
Supervised learning works by tweaking all the parameters (weights in neural networks) to fit the desired outputs, expecting that given enough input/label pairs the AI will find common rules that generalize for any input."><meta itemprop=datePublished content="2021-12-15T00:00:00+00:00"><meta itemprop=dateModified content="2022-01-04T10:49:39+09:00"><meta itemprop=wordCount content="1886"><meta itemprop=keywords content="q-learning,td-learning,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Q-learning: How a Reward Is All You Need"><meta name=twitter:description content="There are two general ways to train an AI to match a given expectation: we can either give it the expected outputs (commonly named labels) for differents inputs; we call this supervised learning. Or we can provide a reward for each output as a score: this is reinforcement learning (RL).
Supervised learning works by tweaking all the parameters (weights in neural networks) to fit the desired outputs, expecting that given enough input/label pairs the AI will find common rules that generalize for any input."><link rel=preload href=/scss/main.min.a648fb06ee87fb08d4b2d5698675182cab33629f1bbc21e5e592a9261834a71d.css as=style><link href=/scss/main.min.a648fb06ee87fb08d4b2d5698675182cab33629f1bbc21e5e592a9261834a71d.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script></head><body class="td-page td-blog"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo></span><span class="text-uppercase font-weight-bold">Spice.ai blog</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://github.com/spiceai/spiceai target=_blank><span>GitHub</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-docs-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-posts-li><a href=/posts/ title="Spice.ai blog" class="align-left pl-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-posts><span>Blog</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20220208announcing-the-release-of-spiceai-v06-alpha-li><a href=/posts/2022/02/08/announcing-the-release-of-spice.ai-v0.6-alpha/ title="Announcing the release of Spice.ai v0.6-alpha" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20220208announcing-the-release-of-spiceai-v06-alpha><span>Spice.ai v0.6-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20220112adding-soft-actor-critic-li><a href=/posts/2022/01/12/adding-soft-actor-critic/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20220112adding-soft-actor-critic><span>Adding Soft Actor-Critic</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20220104what-data-informs-ai-driven-decision-making-li><a href=/posts/2022/01/04/what-data-informs-ai-driven-decision-making/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20220104what-data-informs-ai-driven-decision-making><span>What Data Informs AI-driven Decision Making?</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211230a-new-class-of-applications-that-learn-and-adapt-li><a href=/posts/2021/12/30/a-new-class-of-applications-that-learn-and-adapt/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211230a-new-class-of-applications-that-learn-and-adapt><span>A New Class of Applications That Learn and Adapt</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211228announcing-the-release-of-spiceai-v051-alpha-li><a href=/posts/2021/12/28/announcing-the-release-of-spice.ai-v0.5.1-alpha/ title="Announcing the release of Spice.ai v0.5.1-alpha" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211228announcing-the-release-of-spiceai-v051-alpha><span>Spice.ai v0.5.1-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-posts20211215understanding-q-learning-how-a-reward-is-all-you-need-li><a href=/posts/2021/12/15/understanding-q-learning-how-a-reward-is-all-you-need/ class="align-left pl-0 active td-sidebar-link td-sidebar-link__page" id=m-posts20211215understanding-q-learning-how-a-reward-is-all-you-need><span class=td-sidebar-nav-active-item>Understanding Q-learning: How a Reward Is All You Need</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211206announcing-the-release-of-spiceai-v05-alpha-li><a href=/posts/2021/12/06/announcing-the-release-of-spice.ai-v0.5-alpha/ title="Announcing the release of Spice.ai v0.5-alpha" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211206announcing-the-release-of-spiceai-v05-alpha><span>Spice.ai v0.5-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211205ai-needs-ai-ready-data-li><a href=/posts/2021/12/05/ai-needs-ai-ready-data/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211205ai-needs-ai-ready-data><span>AI needs AI-ready data</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211202spicepods-from-zero-to-hero-li><a href=/posts/2021/12/02/spicepods-from-zero-to-hero/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211202spicepods-from-zero-to-hero><span>Spicepods: From Zero To Hero</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211122announcing-the-release-of-spiceai-v041-alpha-li><a href=/posts/2021/11/22/announcing-the-release-of-spice.ai-v0.4.1-alpha/ title="Announcing the release of Spice.ai v0.4.1-alpha" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211122announcing-the-release-of-spiceai-v041-alpha><span>Spice.ai v0.4.1-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211118spiceais-approach-to-time-series-ai-li><a href=/posts/2021/11/18/spice.ais-approach-to-time-series-ai/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211118spiceais-approach-to-time-series-ai><span>Spice.ai's approach to Time-Series AI</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211115announcing-the-release-of-spiceai-v04-alpha-li><a href=/posts/2021/11/15/announcing-the-release-of-spice.ai-v0.4-alpha/ title="Announcing the release of Spice.ai v0.4-alpha" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211115announcing-the-release-of-spiceai-v04-alpha><span>Spice.ai v0.4-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211115teaching-apps-how-to-learn-with-spicepods-li><a href=/posts/2021/11/15/teaching-apps-how-to-learn-with-spicepods/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211115teaching-apps-how-to-learn-with-spicepods><span>Teaching Apps how to Learn with Spicepods</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211105making-apps-that-learn-and-adapt-li><a href=/posts/2021/11/05/making-apps-that-learn-and-adapt/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211105making-apps-that-learn-and-adapt><span>Making Apps That Learn And Adapt</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211102announcing-the-release-of-spiceai-v031-alpha-li><a href=/posts/2021/11/02/announcing-the-release-of-spice.ai-v0.3.1-alpha/ title="Announcing the release of Spice.ai v0.3.1-alpha" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211102announcing-the-release-of-spiceai-v031-alpha><span>Spice.ai v0.3.1-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211026spiceai-v03-alpha-is-now-available-li><a href=/posts/2021/10/26/spice.ai-v0.3-alpha-is-now-available/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211026spiceai-v03-alpha-is-now-available><span>Spice.ai v0.3-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211012announcing-the-release-of-spiceai-v021-alpha-li><a href=/posts/2021/10/12/announcing-the-release-of-spice.ai-v0.2.1-alpha/ title="Announcing the release of Spice.ai v0.2.1-alpha" class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211012announcing-the-release-of-spiceai-v021-alpha><span>Spice.ai v0.2.1-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20211004spiceai-v02-alpha-is-now-available-li><a href=/posts/2021/10/04/spice.ai-v0.2-alpha-is-now-available/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20211004spiceai-v02-alpha-is-now-available><span>Spice.ai v0.2-alpha is now available</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-posts20210907introducing-spiceai-open-source-time-series-ai-for-developers-li><a href=/posts/2021/09/07/introducing-spice.ai-open-source-time-series-ai-for-developers/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-posts20210907introducing-spiceai-open-source-time-series-ai-for-developers><span>Introducing Spice.ai - open source, time series AI for developers</span></a></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"><a id=print href=/posts/_print/><i class="fa fa-print fa-fw"></i> Print entire section</a></div><div class=td-toc><nav id=TableOfContents><ul><li><a href=#q-learning>Q-learning</a><ul><li><a href=#q-table>Q-Table</a></li><li><a href=#deep-q-learning>Deep Q-Learning</a></li></ul></li><li><a href=#temporal-difference-td-learning>Temporal difference: TD-Learning</a><ul><li><a href=#introducing-policies>Introducing policies</a></li><li><a href=#relation-with-q-function>Relation with Q function</a></li><li><a href=#td-0>TD-0</a></li><li><a href=#td-lambda>TD-lambda</a></li></ul></li><li><a href=#q-learning-algorithm>Q-Learning algorithm</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div><div class="taxonomy taxonomy-terms-cloud taxo-categories"><h5 class=taxonomy-title>Categories</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=/categories/applications/ data-taxonomy-term=applications><span class=taxonomy-label>applications</span><span class=taxonomy-count>4</span></a></li><li><a class=taxonomy-term href=/categories/data/ data-taxonomy-term=data><span class=taxonomy-label>data</span><span class=taxonomy-count>2</span></a></li><li><a class=taxonomy-term href=/categories/data-engineering/ data-taxonomy-term=data-engineering><span class=taxonomy-label>data engineering</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/categories/deep-learning/ data-taxonomy-term=deep-learning><span class=taxonomy-label>deep learning</span><span class=taxonomy-count>2</span></a></li><li><a class=taxonomy-term href=/categories/learn-and-adapt/ data-taxonomy-term=learn-and-adapt><span class=taxonomy-label>learn-and-adapt</span><span class=taxonomy-count>4</span></a></li><li><a class=taxonomy-term href=/categories/reinforcement-learning/ data-taxonomy-term=reinforcement-learning><span class=taxonomy-label>reinforcement learning</span><span class=taxonomy-count>3</span></a></li><li><a class=taxonomy-term href=/categories/release/ data-taxonomy-term=release><span class=taxonomy-label>release</span><span class=taxonomy-count>9</span></a></li><li><a class=taxonomy-term href=/categories/time-series-ai/ data-taxonomy-term=time-series-ai><span class=taxonomy-label>time-series ai</span><span class=taxonomy-count>1</span></a></li></ul></div><div class="taxonomy taxonomy-terms-cloud taxo-tags"><h5 class=taxonomy-title>Tags</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=/tags/actor-critic/ data-taxonomy-term=actor-critic><span class=taxonomy-label>actor-critic</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/agile-ml/ data-taxonomy-term=agile-ml><span class=taxonomy-label>agile ml</span><span class=taxonomy-count>4</span></a></li><li><a class=taxonomy-term href=/tags/arrow/ data-taxonomy-term=arrow><span class=taxonomy-label>arrow</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/categorical-data/ data-taxonomy-term=categorical-data><span class=taxonomy-label>categorical data</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/cnn/ data-taxonomy-term=cnn><span class=taxonomy-label>cnn</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/dashboard/ data-taxonomy-term=dashboard><span class=taxonomy-label>dashboard</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/data/ data-taxonomy-term=data><span class=taxonomy-label>data</span><span class=taxonomy-count>3</span></a></li><li><a class=taxonomy-term href=/tags/dataspaces/ data-taxonomy-term=dataspaces><span class=taxonomy-label>dataspaces</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/decision-engines/ data-taxonomy-term=decision-engines><span class=taxonomy-label>decision engines</span><span class=taxonomy-count>2</span></a></li><li><a class=taxonomy-term href=/tags/decision-trees/ data-taxonomy-term=decision-trees><span class=taxonomy-label>decision trees</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/expert-systems/ data-taxonomy-term=expert-systems><span class=taxonomy-label>expert systems</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/numpy/ data-taxonomy-term=numpy><span class=taxonomy-label>numpy</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/pandas/ data-taxonomy-term=pandas><span class=taxonomy-label>pandas</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/performance/ data-taxonomy-term=performance><span class=taxonomy-label>performance</span><span class=taxonomy-count>2</span></a></li><li><a class=taxonomy-term href=/tags/q-learning/ data-taxonomy-term=q-learning><span class=taxonomy-label>q-learning</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/rewards/ data-taxonomy-term=rewards><span class=taxonomy-label>rewards</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/rnn/ data-taxonomy-term=rnn><span class=taxonomy-label>rnn</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/spicepod/ data-taxonomy-term=spicepod><span class=taxonomy-label>spicepod</span><span class=taxonomy-count>3</span></a></li><li><a class=taxonomy-term href=/tags/td-learning/ data-taxonomy-term=td-learning><span class=taxonomy-label>td-learning</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/tensors/ data-taxonomy-term=tensors><span class=taxonomy-label>tensors</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/training/ data-taxonomy-term=training><span class=taxonomy-label>training</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/transformers/ data-taxonomy-term=transformers><span class=taxonomy-label>transformers</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=/tags/upgrade/ data-taxonomy-term=upgrade><span class=taxonomy-label>upgrade</span><span class=taxonomy-count>2</span></a></li></ul></div></aside><main class="col-12 col-md-9 col-xl-8 pl-md-5 pr-md-4" role=main><div class=td-content><h1>Understanding Q-learning: How a Reward Is All You Need</h1><div class="td-byline mb-4">By <b>Corentin Risselin</b> |
<time datetime=2021-12-15 class=text-muted>Wednesday, December 15, 2021</time></div><header class=article-meta><div class="taxonomy taxonomy-terms-article taxo-categories"><h5 class=taxonomy-title>Categories:</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=/categories/reinforcement-learning/ data-taxonomy-term=reinforcement-learning><span class=taxonomy-label>reinforcement learning</span></a></li><li><a class=taxonomy-term href=/categories/deep-learning/ data-taxonomy-term=deep-learning><span class=taxonomy-label>deep learning</span></a></li></ul></div><div class="taxonomy taxonomy-terms-article taxo-tags"><h5 class=taxonomy-title>Tags:</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=/tags/q-learning/ data-taxonomy-term=q-learning><span class=taxonomy-label>q-learning</span></a></li><li><a class=taxonomy-term href=/tags/td-learning/ data-taxonomy-term=td-learning><span class=taxonomy-label>td-learning</span></a></li></ul></div></header><p>There are two general ways to train an AI to match a given expectation: we can either give it the expected outputs (commonly named labels) for differents inputs; we call this supervised learning. Or we can provide a reward for each output as a score: this is reinforcement learning (RL).</p><p>Supervised learning works by tweaking all the parameters (weights in neural networks) to fit the desired outputs, expecting that given enough input/label pairs the AI will find common rules that generalize for any input.</p><p>Reinforcement learning&rsquo;s reward is often provided from a simple function that can score any output: we don&rsquo;t know what specific output would be best, but we can recognize how good the result is. In this latter statement there are two underlying concepts we will address in this post:</p><ul><li>Can we only tell if the output is good in a binary way, or do we have to quantify the output to train our AI?</li><li>Do we have to give a reward for every AI&rsquo;s output? Can we give a reward only at specific times?</li></ul><p>Those questions are already mostly answered, and many algorithms deal with those topics. Our journey here will be to understand how we tackle those questions and end up with a beautiful formula that is at the core of modern approaches of RL:</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/q_formula.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 1. Q estimation at the heart of many RL algorithm, also known as the Bellman equation.</div></div></div><h2 id=q-learning>Q-learning</h2><p>The vast majority, if not all, of modern RL algorithms are based on the principles of Q-learning: the idea is to evaluate a &lsquo;reward expectation&rsquo; for each possible action. If we can have a good evaluation, we could maximize the reward by choosing actions with the maximum evaluated rewards. The function giving this expected reward is named Q. For now, we will assume we can have a reward for any action.</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/q_function.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 2. Definition of the Q function.</div></div></div><p>The <code>t</code> indices show that the state and action aren&rsquo;t constant and will vary, usually with time/action taken. On the other hand, the <code>Q</code> function and the reward function <code>r</code> are unique functions that ideally return the &rsquo;expected reward&rsquo; for any (state, action) pairs.</p><p>For now, we will assume we can have a reward that gives an objective and perfect evaluation of each state/action.</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;margin:auto src=https://user-images.githubusercontent.com/19952490/145569847-4be91c13-3ffb-4ad8-83c4-fb841e9d2c96.png><div style=font-size:.8rem;font-style:italic;text-align:center>Figure 1. Example of reward given for different actions at a specific state. Here a simple 2D map with a goal.</div></div></div><h3 id=q-table>Q-Table</h3><p>We know that actions&rsquo; outcomes (rewards) will vary depending on the current state we are in, otherwise the problem would be trivial to solve. If the states that are relevant to our actions can be numbered, a simple way would be to build a table with all the possible states/action pairs. There are different ways to build such a table depending on how we can interact with our environment. Eventually, we would have a good &lsquo;map&rsquo; to guide us to do the best actions.</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;margin:auto src=https://user-images.githubusercontent.com/19952490/145569842-298103e3-e7ed-412f-8229-66c745d29807.png><div style=font-size:.8rem;font-style:italic;text-align:center>Figure 2. Example of Q-table: we can build an exhaustive table for all the possible (state, action) pairs</div></div></div><h3 id=deep-q-learning>Deep Q-Learning</h3><p>When the number of variables of the environment relevant to our actions/rewards becomes too large, the number of possible states grows quickly. It doesn&rsquo;t take a lot of possible parameters to make the Q-table approach unfeasible. Neural networks are known to work very nicely and efficiently in high dimensionality (with many input variables). They also generalize well, so the idea in Deep Q-Learning is to use a neural network to predict the different Q values for each action given a state.</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;margin:auto src=https://user-images.githubusercontent.com/19952490/145569840-369d4eb0-48c6-44d8-bc5e-bfabdd7713a4.png><div style=font-size:.8rem;font-style:italic;text-align:center>Figure 3. A neural network can predict Q values from state information</div></div></div><p>In this case, we do not need to give the state/action pairs but only the state, as the neural network would exhaustively return all the Q values associated with each action. Outputting all actions&rsquo; Q value is a common method as the general cases have a complex environment but a smaller number of possible actions.</p><p>This method works very well. It is similar to supervised learning with states as inputs and rewards as labels. We assumed so far that we had a reward for each action, and we chose the next action with the best reward (called a greedy policy). In many cases this is not enough: even if an action would yield the best reward at a given state, this may affect the next state so that we wouldn&rsquo;t optimize the reward in the long term. Also, if we can&rsquo;t have a reward for each action, we usually give 0 as a reward. We will not be able to choose the right action if they affect later states despite not yielding different rewards at the current state.</p><p>The sparsity of rewards or the long-term calculation of total reward (non-greedy policies) leads us to diverge from supervised learning and learn potential future rewards.</p><h2 id=temporal-difference-td-learning>Temporal difference: TD-Learning</h2><p>TD-learning is a clever way to account for potential future value without knowing them yet. TD is a model-free class of algorithms: it does not simulate future states. The main idea is to consider all the rewards of a sequence of actions to give a better value than just the reward of the next action.</p><p>We can, for instance, sum all the future rewards:</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;margin:auto src=https://user-images.githubusercontent.com/19952490/145569849-f528b7df-a240-41d6-b850-fde58334cac5.png><div style=font-size:.8rem;font-style:italic;text-align:center>Figure 4. Cumulating future rewards to assign values to each state.</div></div></div><p>Mathematically this can be written as:</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/value_naive_function.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 3.</div></div></div><p>This is named TD(0): the simplest form of TD method, accumulating all the rewards.</p><h3 id=introducing-policies>Introducing policies</h3><p>We could try different trajectories (sequence of actions) and retrospectively get the final reward for each action, but this has 2 drawbacks: the environment is usually too vast, and the sequence of actions might not even have a definite end. Also, such exhaustive methods might not be very efficient. Instead, we can evaluate the &lsquo;value&rsquo; of the next state overall, like the maximum of all its possible rewards (direct reward), and add this value to the reward of a given action.</p><p>If a state can have different branches, we can select the best one, and this would be our policy, the way we choose actions. This simple form of taking the maximum is called the &lsquo;greedy&rsquo; policy.</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;margin:auto src=https://user-images.githubusercontent.com/19952490/145569828-f9505a88-1556-4c88-ba43-834daa60e594.png><div style=font-size:.8rem;font-style:italic;text-align:center>Figure 5. With a greedy policy the associated values to state come from the maximum value of the next state. Here despite the lower branch giving only half the top reward directly the overall value is greater.</div></div></div><p>This can be written down as:</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/value_policy_function.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 4.</div></div></div><p>The expected value notation is defined as:</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/expected_value.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 5.</div></div></div><p>For a greedy policy the probabilities <code>p</code> would all be set to 0 but the one associated with the highest return to 1 (in case of equality between n actions, we would attribute &lsquo;1/n&rsquo; as probabilities to get the same expected value).</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/expected_greedy_value.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 6.</div></div></div><h3 id=relation-with-q-function>Relation with Q function</h3><p>The expected reward can be replaced by the Q function we used earlier, which now can be denominated to be specific to our chosen policy (named π):</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/value_q_relation.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 7.</div></div></div><h3 id=td-0>TD-0</h3><p>We previously discussed the problem of not being able to go through all the states exhaustively and that the evaluation of the Q value from a neural network could help. We want to use the TD method to have a better value estimation that will consider potential future rewards.</p><p>The TD(0) method is elegant as we can, in fact, only use the next state&rsquo;s expected value instead of all future ones. The idea is that with successive evaluations, we build a chain of dependencies as each states&rsquo; value depends on the next one.</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/td_0_value.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 8.</div></div></div><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;margin:auto src=https://user-images.githubusercontent.com/19952490/145569853-335f65d9-aa16-44c6-9e97-287db5862628.png><div style=font-size:.8rem;font-style:italic;text-align:center>Figure 6. Iterative propagation of state values following TD(0) method.</div></div></div><p>We can see that the greedy policy would work even with null rewards in the trajectory. We can explicit our greedy policy, going back to use Q value instead of the state value V:</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/td_0_q.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 9.</div></div></div><h3 id=td-lambda>TD-lambda</h3><p>We need to fix a problem: if a trajectory grows too long or never ends, a state value can potentially grow indefinitely. To counter that, we can add a <strong>discount factor</strong> (originally named lambda, usually refer as gamma in Q-learning) for the next state&rsquo;s value:</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;min-height:100px;margin:auto src=/svg/q_learning/td_lambda_q.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 10.</div></div></div><p>Notice that we simplify the reward notation for clarity.</p><p>To avoid exploding values, this discount has to be between 0 and 1 (strictly below 1). We can think about it as giving more importance to the direct reward than the future ones. As the contribution to the latter reward decrease, the chain of action can grow without the calculated value growing. If the reward has an upper limit, the value will also be bounded.</p><p>The sparsity of rewards is also solved: giving only a positive reward after many non-rewarding steps will create smooth values for the intermediate states. Any reward, positive or negative, will diffuse its value to the neighbor states.</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:600px;margin:auto src=https://user-images.githubusercontent.com/19952490/145569835-ff21b42f-21d0-4eb3-a451-9b9aa5a76f78.png><div style=font-size:.8rem;font-style:italic;text-align:center>Figure 7. The TD(0) value propagation can allow for a smooth value distribution over the state that will help building efficient behaviour.</div></div></div><h2 id=q-learning-algorithm>Q-Learning algorithm</h2><p>Finally, as we train a neural network to estimate the Q function, we need to update its target with successive iteration. We cannot fully trust the estimator (a neural network here) to give the correct value, so we introduce a learning rate to update the target smoothly.</p><div style=display:flex;justify-content:center;padding:5px><div style=display:flex;flex-direction:column><img style=max-width:800px;min-height:160px;margin:auto src=/svg/q_learning/final_formula.svg><div style=font-size:.8rem;font-style:italic;text-align:center>Equation 11. Fully explained Bellman equation.</div></div></div><p>That is it! We now understand all the parts of this formula. Over multiple training steps with different sates, the training should find a good average Q function. While training, the estimator uses its own output to train itself (commonly referred to as bootstrapping): it is like it is chasing itself. Bootstrapping can lead to instability in the training process. There are many additional methods to help against such instability.</p><p>From giving rewards, sparse or not, binary or fine-grained, we have a smooth space of values for all our states/actions so the AI can follow a greedy policy to the best outcome.</p><p>This way of training is not a silver bullet and there is no guarantee that the AI will find a correlation from the information given as state to the returned reward.</p><h2 id=conclusion>Conclusion</h2><p>We can see how our rewards are used to train AI&rsquo;s policies using Q-learning. By understanding the many iterations required and the bootstrapping issues, we can help our AI by carefully giving relevant state information and reward:</p><ul><li>There needs to be a correlation between the state information and the reward: the simpler the relationship, the easier/faster the AI will find it.</li><li>Sparse and binary rewards make the training problem long and arduous. Giving more information through the reward can tremendously increase the speed/accuracy of the learned Q-estimator.</li><li>The longer the chain of actions, the more complex the Q-value will be to estimate.</li></ul><p>We didn&rsquo;t see how the AI&rsquo;s algorithm can explore different actions given an environment here. Spice.ai&rsquo;s technology focuses exclusively on off-policy training where we only have past data and cannot interact with the environment. RL is a vast topic and currently quickly growing. Robotics is a fantastic field of application; many other areas are yet to be explored with such a technology. We hope to push forward the technology and its field of application with our platform.</p><p>If you&rsquo;d like to partner with us on the mission of making new applications by leveraging RL, we invite you to discuss with us on <a href=https://discord.gg/kZnTfneP5u>Discord</a>, reach out on <a href=https://twitter.com/SpiceAIHQ>Twitter</a> or <a href=mailto:hey@spiceai.io>email us</a>.</p><p>I hope you enjoy this post and learn new things.</p><p>Corentin</p><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><li><a href=/posts/2021/12/06/announcing-the-release-of-spice.ai-v0.5-alpha/ aria-label="Previous - Announcing the release of Spice.ai v0.5-alpha" class="btn btn-primary"><span class=mr-1>←</span> Previous</a></li><a href=/posts/2021/12/28/announcing-the-release-of-spice.ai-v0.5.1-alpha/ aria-label="Next - Announcing the release of Spice.ai v0.5.1-alpha" class="btn btn-primary">Next <span class=ml-1>→</span></a></li></ul></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel=noopener href=https://twitter.com/SpiceAIHQ aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Discord aria-label=Discord><a class=text-white target=_blank rel=noopener href=https://discord.com/channels/803820740868571196/803820740868571199 aria-label=Discord><i class="fab fa-discord"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Reddit aria-label=Reddit><a class=text-white target=_blank rel=noopener href=https://reddit.com/r/spiceai aria-label=Reddit><i class="fab fa-reddit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/spiceai/spiceai aria-label=GitHub><i class="fab fa-github"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2022 Spice AI, Inc. All Rights Reserved</small></div></div></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js integrity=sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7787e1ad8f3d8a7b6a03ca6abbe35cc6de2a328cc8ec33a32d2df5de9dc644fe.js integrity="sha256-d4fhrY89intqA8pqu+Ncxt4qMozI7DOjLS313p3GRP4=" crossorigin=anonymous></script></body></html>