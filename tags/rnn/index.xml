<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spice.ai blog â€“ rnn</title><link>/tags/rnn/</link><description>Recent content in rnn on Spice.ai blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 18 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/rnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Posts: Spice.ai's approach to Time-Series AI</title><link>/posts/2021/11/18/spice.ais-approach-to-time-series-ai/</link><pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate><guid>/posts/2021/11/18/spice.ais-approach-to-time-series-ai/</guid><description>
&lt;p>The Spice.ai project strives to help developers build applications that leverage new AI advances which can be easily trained, deployed, and integrated. &lt;a href="/posts/2021/11/15/teaching-apps-how-to-learn-with-spicepods/">A previous blog post&lt;/a> introduced Spicepods: a declarative way to create AI applications with Spice.ai technology. While there are many libraries and platforms in the space, Spice.ai is focused on time-series data aligning to application-centric and frequently time-dependent data, and a &lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning&lt;/a> approach, which can be more developer-friendly than expensive, labeled &lt;a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning&lt;/a>.&lt;/p>
&lt;p>This post will discuss some of the challenges and directions for the technology we are developing.&lt;/p>
&lt;h3 id="time-series">Time Series&lt;/h3>
&lt;div style="display: flex; justify-content: center; padding: 5px;">
&lt;div style="display: flex; flex-direction: column;">
&lt;img style="max-width: 600px; margin: auto" src="https://user-images.githubusercontent.com/19952490/142404970-de910848-cdb4-451b-a0d5-302c90215216.png" />
&lt;div style="font-size: 0.8rem; font-style: italic;">Figure 1. Time Series processing visualization: a time window is usually chosen to process part of the data stream&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Time series AI has become more popular over recent years, and there is extensive literature on the subject, including time-series-focused neural networks. Research in this space points to the likelihood that there is no silver bullet, and a single approach to time series AI will not be sufficient. However, for developers, this can make building a product complex, as it comes with the challenge of exploring and evaluating many algorithms and approaches.&lt;/p>
&lt;p>A fundamental challenge of time series is the data itself. The shape and length are usually variable and can even be infinite (real-time streams of data). The volume of data required is often too much for simple and efficient machine learning algorithms such as &lt;a href="https://en.wikipedia.org/wiki/Decision_tree">Decision Trees&lt;/a>. This challenge makes Deep Learning popular to process such data. There are several types of neural networks that have been shown to work well with time series so let&amp;rsquo;s review some of the common classes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Networks (CNN)&lt;/a>: CNN&amp;rsquo;s can only accept data with fixed lengths: even with the ability to pad the data, this is a major drawback for time-series data as a specific time window needs to be decided. Despite this limitation, they are the most efficient network to train (computation, data needed, time) and usually the smallest storage. CNN&amp;rsquo;s are very robust and used in image/video processing, making them a very good baseline to start with while also benefiting from refined and mature development over the years, such as with the very efficient MobileNet with depth-wise convolutions.&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Networks (RNN)&lt;/a>: RNNs have been researched for several decades, and while they aren&amp;rsquo;t as fast to train as CNNs, they can be faster to apply as there is no need to feed a time window like CNNs if the desired input/output is in real-time (in a continuous fashion, also called &amp;lsquo;online). RNNs are proven to be very good in some situations, and many new models are being discovered.&lt;/li>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformers&lt;/a>: Most of the state-of-the-art results today have been made from transformers and their variations. They are very good at correlating sparse information. Popularized in the famous paper &lt;a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need&lt;/a>, transformers are proven to be flexible with high-performance in many classes (Vision Transformers, Perceiver, etc.). They suffer the same limitation as CNNs for the length of their input (fixed at training time), but they also have a disadvantage of not scaling well with the size of the data (quadratic growth with the length of the time series). They are also the most expensive network to train in general.&lt;/li>
&lt;/ul>
&lt;p>While not a complete representation of classes of neural networks, this list represents the areas of the most potential for Spice.ai&amp;rsquo;s time-series AI technology. We also see other interesting paradigms to explore when improving the core technology like Memory Augmented Neural Networks (MANN) or neural network-based Genetical Algorithms.&lt;/p>
&lt;h3 id="reinforcement-learning">Reinforcement Learning&lt;/h3>
&lt;p>Reinforcement Learning (RL) has grown steadily, especially in fields like robotics. Usually, RL doesn&amp;rsquo;t require as much data processing as Supervised Learning, where large datasets can be demanding for hardware and people alike. RL is more dynamic: agents aren&amp;rsquo;t trained to replicate a specific behaviors/output but explore and &amp;rsquo;exploit&amp;rsquo; their environment to maximize a given reward.&lt;/p>
&lt;p>Most of today&amp;rsquo;s research is based on environments the agent can interact with during the training process, known as online learning. Usually, efficient training processes have multiple agent/environment pairs training together and sharing their experiences. Having an environment for agents to interact enables different actions from the actual historical state known as &lt;strong>on-policy learning&lt;/strong>, and using only past experiences without an environment is &lt;strong>off-policy learning&lt;/strong>.&lt;/p>
&lt;div style="display: flex; justify-content: center; padding: 5px;">
&lt;div style="display: flex; flex-direction: column;">
&lt;img style="max-width: 600px; margin: auto" src="https://user-images.githubusercontent.com/19952490/142404987-cc6f0654-d2bd-496a-b6a4-52da19b9f912.png" />
&lt;div style="font-size: 0.8rem; font-style: italic;"> Figure 2. AI training without interacting with the environment (real world nor simulation). Only gathered data is used for training.&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Spice.ai is initially taking an off-policy approach, where an environment (either pre-made or given by the user) is not required. Despite limiting the exploration of agents, this aligns to an application-centric approach as:&lt;/p>
&lt;ul>
&lt;li>Creating a real-world model or environment can be difficult and expensive to create, arguably even impossible.&lt;/li>
&lt;li>Off-policy learning is normally more efficient than on-policy (time/data and computation).&lt;/li>
&lt;/ul>
&lt;p>The Spice.ai approach to time series AI can be described as &amp;lsquo;Data-Driven&amp;rsquo; Reinforcement Learning. This domain is very exciting, and we are building upon excellent research that is being published. The &lt;a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence Research&lt;/a>&amp;rsquo;s blog shows the potential of this field and many other research entities that have made great discoveries like &lt;a href="https://deepmind.com/">DeepMind&lt;/a>, &lt;a href="https://openai.com/">Open AI&lt;/a>, &lt;a href="https://ai.facebook.com/">Facebook AI&lt;/a> and &lt;a href="https://ai.google/">Google AI&lt;/a> (among many others). We are inspired and are building upon all the research in Reinforcement Learning to develop core Spice.ai technology.&lt;/p>
&lt;p>If you are interested in Reinforcement Learning, we recommend following these blogs, and if you&amp;rsquo;d like to partner with us on the mission of making it easier to build intelligent applications by leveraging RL, we invite you to discuss with us on &lt;a href="https://discord.gg/kZnTfneP5u">Discord&lt;/a>, reach out on &lt;a href="https://twitter.com/SpiceAIHQ">Twitter&lt;/a> or &lt;a href="mailto:hey@spiceai.io">email us&lt;/a>.&lt;/p>
&lt;p>Corentin&lt;/p></description></item></channel></rss>